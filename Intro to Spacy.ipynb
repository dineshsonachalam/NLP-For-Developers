{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Spacy\n",
    "\n",
    "<b>spaCy</b> is a framework for NLP written in Cython and super fast when compared to NLTK framwork\n",
    "\n",
    "#### Advantages:\n",
    "1. Super fast\n",
    "2. Its accurate\n",
    "3. Pretrained word vectors\n",
    "4. Beautiful visualizations\n",
    "5. It also has its own DL framwork for NLP tasks\n",
    "\n",
    "#### Installation:\n",
    "pip install -U spaCy\n",
    "\n",
    "#### Note:\n",
    "Notice that the installation doesn’t automatically download the English model. We need to do that ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Warning: no model found for 'en'\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n",
      "\"Hello\" 0\n",
      "\"    \" 6\n",
      "\"world\" 10\n",
      "\"!\" 15\n"
     ]
    }
   ],
   "source": [
    "# How to get index of a word in spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en') # Load the english model\n",
    "doc = nlp('Hello     world!')\n",
    "for token in doc:\n",
    "    print('\"' + token.text + '\"', token.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next\t0\t\tFalse\tFalse\tXxxx\t\t\n",
      "week\t5\t\tFalse\tFalse\txxxx\t\t\n",
      "I\t10\t-PRON-\tFalse\tFalse\tX\tPRON\tPRP\n",
      "'ll\t11\twill\tFalse\tFalse\t'xx\tVERB\tMD\n",
      "  \t15\t\tFalse\tTrue\t  \t\t\n",
      "be\t17\t\tFalse\tFalse\txx\t\t\n",
      "in\t20\t\tFalse\tFalse\txx\t\t\n",
      "Madrid\t23\t\tFalse\tFalse\tXxxxx\t\t\n",
      ".\t29\t\tTrue\tFalse\t.\t\t\n"
     ]
    }
   ],
   "source": [
    "# here Token class exposes a lot of word-level attributes. Here are a few examples:\n",
    "doc = nlp(\"Next week I'll   be in Madrid.\")\n",
    "for token in doc:\n",
    "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n",
    "        token.text,\n",
    "        token.idx,\n",
    "        token.lemma_,\n",
    "        token.is_punct,\n",
    "        token.is_space,\n",
    "        token.shape_,\n",
    "        token.pos_,\n",
    "        token.tag_\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n",
      "This is another one.\n",
      "And this is a third, for good measure.\n"
     ]
    }
   ],
   "source": [
    "# Sentence detection\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "r = nlp(u'This is a sentence. This is another one. And this is a third, for good measure.' )\n",
    "for s in r.sents:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Next', 'JJ'), ('week', 'NN'), ('I', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('in', 'IN'), ('Madrid', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Part Of Speech Tagging\n",
    "doc = nlp(\"Next week I'll be in Madrid.\")\n",
    "print([(token.text, token.tag_) for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition:\n",
    "Doing NER with spaCy is super easy and the pretrained model performs pretty well:\n",
    "\n",
    "Spacy consists of a fast entity recognition model which is capable of identifying entitiy phrases from the document. Entities can be of different types, such as – person, location, organization, dates, numerals, etc. These entities can be accessed through “.ents” property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 CARDINAL\n",
      "9 a.m. because the stock went up 30% in just 2 days according to the WSJ TIME\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    " \n",
    "# Next week DATE\n",
    "# Madrid GPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall/NNP <--compound-- Journal/NNP\n",
      "Street/NNP <--compound-- Journal/NNP\n",
      "Journal/NNP <--nsubj-- published/VBD\n",
      "just/RB <--advmod-- published/VBD\n",
      "published/VBD <--ROOT-- published/VBD\n",
      "an/DT <--det-- piece/NN\n",
      "interesting/JJ <--amod-- piece/NN\n",
      "piece/NN <--dobj-- published/VBD\n",
      "on/IN <--prep-- piece/NN\n",
      "crypto/JJ <--amod-- currencies/NNS\n",
      "currencies/NNS <--pobj-- on/IN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Wall Street Journal just published an interesting piece on crypto currencies')\n",
    " \n",
    "for token in doc:\n",
    "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n",
    "        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors\n",
    "The vectors are attached to spaCy objects: Token, Lexeme (a sort of unnatached token, part of the vocabulary), Span and Doc. The multi-token objects average its constituent vectors.\n",
    "\n",
    "<b>Few properties word vectors have:</b>\n",
    "\n",
    "1. If two words are similar, they appear in similar contexts\n",
    "2. Word vectors are computed taking into account the context (surrounding words)\n",
    "3. Given the two previous observations, similar words should have similar word vectors\n",
    "4. Using vectors we can derive relationships between words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing similarity\n",
    "It is able to compute the similarity between words.(Token, Span, Doc and Lexeme.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Architecture\n",
    "The entire spaCy architecture is built upon three building blocks: Document (the big encompassing container), Token(most of the time, a word) and Span (set of consecutive Tokens). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning\n",
    "spaCy is the best way to prepare text for deep learning. It interoperates seamlessly with TensorFlow, PyTorch, scikit-learn, Gensim and the rest of Python's awesome AI ecosystem. With spaCy, you can easily construct linguistically sophisticated statistical models for a variety of NLP problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
